{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install html2text\n",
    "%pip install html2text==2024.2.26\n",
    "#Installing bs4 package\n",
    "%pip install bs4==0.0.2 \n",
    "#Installing lxml\n",
    "%pip install lxml==5.2.2 \n",
    "# Install the Sentence Transformers library\n",
    "%pip install sentence_transformers==2.7.0 \n",
    "# Install FAISS-CPU\n",
    "%pip install faiss-cpu #==1.8.0\n",
    "%pip install faiss-gpu\n",
    "%pip install langchain_huggingface\n",
    "%pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the AsyncHtmlLoader\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "#This is the url of the wikipedia page on the 2023 Cricket World Cup\n",
    "url=\"https://en.wikipedia.org/wiki/2023_Cricket_World_Cup\"\n",
    "\n",
    "#Invoking the AsyncHtmlLoader\n",
    "loader = AsyncHtmlLoader (url)\n",
    "\n",
    "#Loading the extracted information\n",
    "data = loader.load()\n",
    "\n",
    "\n",
    "\n",
    "#Import Html2TextTransformer\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "#Assign the Html2TextTransformer function\n",
    "html2text = Html2TextTransformer()\n",
    "\n",
    "#Call transform_documents\n",
    "data_transformed = html2text.transform_documents(data)\n",
    "\n",
    "print(data_transformed[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the HTMLHeaderTextSplitter library\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "# Set url as the Wikipedia page link\n",
    "url=\"https://en.wikipedia.org/wiki/2023_Cricket_World_Cup\"\n",
    "\n",
    "# Specify the header tags on which splits should be made\n",
    "headers_to_split_on=[\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\")\n",
    "]\n",
    "\n",
    "# Create the HTMLHeaderTextSplitter function\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# Create splits in text obtained from the url\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(html_header_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Conversion (Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HuggingFaceEmbeddings from embeddings library\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Instantiate the embeddings model. The embeddings model_name can be changed as desired\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-l6-v2\")\n",
    "\n",
    "# Create embeddings for all chunk\n",
    "chunk_embedding = embeddings.embed_documents([chunk.page_content for chunk in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FAISS class from vectorstore library\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create the database\n",
    "db=FAISS.from_documents(chunks,hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic test dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the AsyncHtmlLoader\n",
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "#This is the url of the wikipedia page on the 2023 Cricket World Cup\n",
    "url=\"https://en.wikipedia.org/wiki/2023_Cricket_World_Cup\"\n",
    "\n",
    "#Instantiating the AsyncHtmlLoader\n",
    "loader = AsyncHtmlLoader (url)\n",
    "\n",
    "#Loading the extracted information\n",
    "data = loader.load()\n",
    "\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "#Instantiate the Html2TextTransformer function\n",
    "html2text = Html2TextTransformer()\n",
    "\n",
    "#Call transform_documents\n",
    "data_transformed = html2text.transform_documents(data)\n",
    "\n",
    "# Import necessary libraries\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Instantiate the models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create the TestsetGenerator\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# Call the generator\n",
    "testset = generator.generate_with_langchain_docs(\n",
    "data_transformed, \n",
    "test_size=20, \n",
    "distributions={ \n",
    "simple: 0.5, \n",
    "reasoning: 0.25, \n",
    "multi_context: 0.25}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG agent with CrewAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install crewai crewai_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Crew, Task, Agent, LLM\n",
    "from crewai_tools import RagTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"openai/gpt-4\", max_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"llm\": {\n",
    "        \"provider\": \"openai\", \n",
    "        \"config\": {\n",
    "            \"model\": \"gpt-4\",\n",
    "        }\n",
    "    },\n",
    "    \"embedding_model\": {\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\n",
    "            \"model\": \"text-embedding-ada-002\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_tool = RagTool(config=config,  \n",
    "    chunk_size=1200,       \n",
    "    chunk_overlap=200,     \n",
    ")\n",
    "rag_tool.add(\"Structured_AI_development.pdf\", data_type=\"pdf_file\")\n",
    "#\"model_cards/facebook/bart-large-mnli/README.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
