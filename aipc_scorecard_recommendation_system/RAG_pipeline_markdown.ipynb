{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda0c2a-2b3c-47c2-a5f1-b779389dc7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq pypdf faiss-gpu pandas SQLAlchemy langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f59f28-1e6c-4f80-b501-aab12c812d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from urllib.request import urlretrieve\n",
    "from sqlalchemy import create_engine\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59241b21-a406-4485-87aa-87f8c40458ba",
   "metadata": {},
   "source": [
    "# Persist prompt-response experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98d49a53-f26f-47a8-ab48-3c96ee34738c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>llm</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>relevant_documents</th>\n",
       "      <th>datetime</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, response, llm, embedding_model, relevant_documents, datetime, source]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run the first time we create the db of prompts\n",
    "engine = create_engine('sqlite:///prompts_history.db')\n",
    "table_name = 'prompts'\n",
    "columns = [\"prompt\", \"response\", \"llm\", \"embedding_model\", \"relevant_documents\", \"datetime\", \"source\"]\n",
    "if not os.path.isfile(\"prompts_history.db\"):\n",
    "    row = [{el:\"\" for el in columns}]\n",
    "    prompts_df = pd.DataFrame(columns=columns)\n",
    "else:\n",
    "    query = \"SELECT * FROM prompts\"\n",
    "    prompts_df = pd.read_sql(query, engine)\n",
    "prompts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54762c3-4124-48b5-abbe-af869709c7e5",
   "metadata": {},
   "source": [
    "# Preare documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f98e18-cd89-4b5f-b252-2a6c8ab2f861",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bb1f0a6-3db3-430b-89aa-f37abfd553e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "loader = DirectoryLoader('model_cards/', glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b39643-da2d-42c5-9d65-fa227d4794e0",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8795d632-2b17-4abe-9762-c5cef1079780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=16000,\n",
    "                                               chunk_overlap=2000,\n",
    "                                               separators=['\\n', '.'])\n",
    "docs_before_split = loader.load()\n",
    "docs_after_split = text_splitter.split_documents(docs_before_split)\n",
    "len(docs_after_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8648dbc-de8a-4eb2-ad11-7b60a271b6fa",
   "metadata": {},
   "source": [
    "Documents should be:\n",
    "\n",
    "- large enough to contain enough information to answer a question, and\n",
    "- small enough to fit into the LLM prompt: Mistral-7B-v0.1 input tokens limited to 4096 tokens\n",
    "- small enough to fit into the embeddings model: BAAI/bge-small-en-v1.5: input tokens limited to 512 tokens (roughly 2000 characters. Note: 1 token ~ 4 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52f79293-bbfa-4482-86e1-e49e3a1c95d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split, there were 283 documents loaded, with average characters equal to 2482.\n",
      "After split, there were 285 documents (chunks), with average characters equal to 2491 (average chunk length).\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)\n",
    "avg_char_before_split = avg_doc_length(docs_before_split)\n",
    "avg_char_after_split = avg_doc_length(docs_after_split)\n",
    "\n",
    "print(f'Before split, there were {len(docs_before_split)} documents loaded, with average characters equal to {avg_char_before_split}.')\n",
    "print(f'After split, there were {len(docs_after_split)} documents (chunks), with average characters equal to {avg_char_after_split} (average chunk length).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0e9a3e-11cb-4cc5-85a3-ed57796cdbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e8d0991-f4f1-4059-81ec-c2476972f095",
   "metadata": {},
   "source": [
    "# Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5ac13b7-42d5-4e84-8d46-81587ddd2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",  # alternatively use \"sentence-transformers/all-MiniLM-l6-v2\" for a light and faster experience.\n",
    "    model_kwargs={'device':'cpu'}, \n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8f04cf4-d57b-496d-a1d3-685a78d13d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the embedding:  (384,)\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(huggingface_embeddings.embed_query(docs_after_split[0].page_content))\n",
    "#print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdeec4a-e181-4356-8ea7-762ebd46ffda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd40c92-353c-428f-98d6-fffa7511a7a1",
   "metadata": {},
   "source": [
    "# INDEXING"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa0a9ff4-6788-48ed-ac88-71034b9fb966",
   "metadata": {},
   "source": [
    "Once we have a embedding model, we are ready to vectorize all our documents and store them in a vector store to construct a retrieval system. With specifically designed searching algorithms, a retrieval system can do similarity searching efficiently to retrieve relevant documents.\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves limitations of traditional query search engines that are optimized for hash-based searches, and provides more scalable similarity search functions (nearest-neighbor search implementations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43383945-f429-4704-90d4-c9cbef0c6c54",
   "metadata": {},
   "source": [
    "### FAISS as a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e51627bb-1296-4c0e-bd06-18ecb051ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs_after_split, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e3143f0b-c88b-4287-a7f2-699ac796847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"What is the model that can be used for legal document classification?\"\"\"  \n",
    "# similarity_search_with_score\n",
    "relevant_documents = vectorstore.similarity_search(query)\n",
    "#relevant_documents\n",
    "vectorstore2 = FAISS.from_documents(relevant_documents, huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "445bc460-4ea1-4e6b-bf1e-d5b881f6837b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='---\\ntags:\\n- text-classification\\n- bert\\n---\\n\\n# Model Card for bleurt-tiny-512 \\n \\n# Model Details\\n \\n## Model Description\\n \\nPytorch version of the original BLEURT models from ACL paper\\n \\n- **Developed by:** Elron Bandel, Thibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research\\n- **Shared by [Optional]:** Elron Bandel\\n- **Model type:** Text Classification \\n- **Language(s) (NLP):** More information needed\\n- **License:** More information needed \\n- **Parent Model:** BERT\\n- **Resources for more information:**\\n     - [GitHub Repo](https://github.com/google-research/bleurt/tree/master)\\n \\t  - [Associated Paper](https://aclanthology.org/2020.acl-main.704/)\\n    - [Blog Post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)\\n \\t\\n\\n\\n# Uses\\n \\n\\n## Direct Use\\nThis model can be used for the task of Text Classification \\n \\n## Downstream Use [Optional]\\n \\nMore information needed.\\n \\n## Out-of-Scope Use\\n \\nThe model should not be used to intentionally create hostile or alienating environments for people. \\n \\n# Bias, Risks, and Limitations\\n \\n \\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\\n\\n\\n\\n## Recommendations\\n \\n \\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\\n\\n# Training Details\\n \\n## Training Data\\nThe model authors note in the [associated paper](https://aclanthology.org/2020.acl-main.704.pdf): \\n> We use years 2017 to 2019 of the WMT Metrics Shared Task, to-English language pairs. For each year, we used the of- ficial WMT test set, which include several thou- sand pairs of sentences with human ratings from the news domain. The training sets contain 5,360, 9,492, and 147,691 records for each year. \\n \\n \\n## Training Procedure\\n\\n \\n### Preprocessing\\n \\nMore information needed \\n \\n### Speeds, Sizes, Times\\nMore information needed \\n\\n \\n# Evaluation\\n \\n \\n## Testing Data, Factors & Metrics\\n \\n### Testing Data\\n \\nThe test sets for years 2018 and 2019 [of the WMT Metrics Shared Task, to-English language pairs.]  are noisier,\\n \\n \\n \\n### Factors\\nMore information needed\\n \\n### Metrics\\n \\nMore information needed\\n \\n \\n## Results \\n \\nMore information needed\\n\\n \\n# Model Examination\\n \\nMore information needed\\n \\n# Environmental Impact\\n \\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\\n \\n- **Hardware Type:** More information needed\\n- **Hours used:** More information needed\\n- **Cloud Provider:** More information needed\\n- **Compute Region:** More information needed\\n- **Carbon Emitted:** More information needed\\n \\n# Technical Specifications [optional]\\n \\n## Model Architecture and Objective\\n\\nMore information needed \\n \\n## Compute Infrastructure\\n \\nMore information needed \\n \\n### Hardware\\n \\n \\nMore information needed\\n \\n### Software\\n \\nMore information needed.\\n \\n# Citation\\n\\n \\n**BibTeX:**\\n \\n \\n```bibtex\\n@inproceedings{sellam2020bleurt,\\n  title = {BLEURT: Learning Robust Metrics for Text Generation},\\n  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},\\n  year = {2020},\\n  booktitle = {Proceedings of ACL}\\n}\\n```\\n \\n \\n \\n \\n# Glossary [optional]\\nMore information needed \\n \\n# More Information [optional]\\nMore information needed \\n\\n \\n# Model Card Authors [optional]\\n \\n Elron Bandel in collaboration with Ezi Ozoani and the Hugging Face team\\n\\n\\n# Model Card Contact\\n \\nMore information needed\\n \\n# How to Get Started with the Model\\n \\nUse the code below to get started with the model.\\n \\n<details>\\n<summary> Click to expand </summary>\\n\\n```python\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\nimport torch\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-tiny-512\")\\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-tiny-512\")\\nmodel.eval()\\n\\nreferences = [\"hello world\", \"hello world\"]\\ncandidates = [\"hi universe\", \"bye world\"]\\n\\nwith torch.no_grad():\\n  scores = model(**tokenizer(references, candidates, return_tensors=\\'pt\\'))[0].squeeze()\\n\\nprint(scores) # tensor([-0.9414, -0.5678])\\n ```\\n\\nSee [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing) for model conversion code. \\n</details>', metadata={'source': 'model_cards/Elron/bleurt-tiny-512/README.md'})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa4f9856-1802-4e32-98e5-c986a7d76e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>llm</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>relevant_documents</th>\n",
       "      <th>datetime</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\ntags:\\n- text-classificatio...</td>\n",
       "      <td>2024-06-19 14:29:28.161965</td>\n",
       "      <td>model_cards/Elron/bleurt-tiny-512/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\ninference: false\\nlicense: ...</td>\n",
       "      <td>2024-06-19 14:29:28.161965</td>\n",
       "      <td>model_cards/Fujitsu/AugCode/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\nlanguage: en\\nlicense: apac...</td>\n",
       "      <td>2024-06-19 14:29:28.161965</td>\n",
       "      <td>model_cards/Hate-speech-CNERG/bert-base-uncase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\nlanguage:\\n- da\\nlicense: a...</td>\n",
       "      <td>2024-06-19 14:29:28.161965</td>\n",
       "      <td>model_cards/alexandrainst/da-ned-base/README.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt response llm  \\\n",
       "0  What is the model that can be used for legal d...                \n",
       "1  What is the model that can be used for legal d...                \n",
       "2  What is the model that can be used for legal d...                \n",
       "3  What is the model that can be used for legal d...                \n",
       "\n",
       "  embedding_model                                 relevant_documents  \\\n",
       "0                  page_content='---\\ntags:\\n- text-classificatio...   \n",
       "1                  page_content='---\\ninference: false\\nlicense: ...   \n",
       "2                  page_content='---\\nlanguage: en\\nlicense: apac...   \n",
       "3                  page_content='---\\nlanguage:\\n- da\\nlicense: a...   \n",
       "\n",
       "                    datetime  \\\n",
       "0 2024-06-19 14:29:28.161965   \n",
       "1 2024-06-19 14:29:28.161965   \n",
       "2 2024-06-19 14:29:28.161965   \n",
       "3 2024-06-19 14:29:28.161965   \n",
       "\n",
       "                                              source  \n",
       "0        model_cards/Elron/bleurt-tiny-512/README.md  \n",
       "1              model_cards/Fujitsu/AugCode/README.md  \n",
       "2  model_cards/Hate-speech-CNERG/bert-base-uncase...  \n",
       "3    model_cards/alexandrainst/da-ned-base/README.md  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = datetime.now()\n",
    "for doc in relevant_documents:\n",
    "    prompts_df.loc[len(prompts_df)] = {\n",
    "        \"prompt\": query, \n",
    "        \"response\": \"\", \n",
    "        \"llm\": \"\", \n",
    "        \"embedding_model\": \"\",\n",
    "        \"relevant_documents\": str(doc), \n",
    "        \"datetime\": date,\n",
    "        \"source\": str(doc.metadata[\"source\"])}\n",
    "prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73cdfdef-a9bb-4321-bd22-db7c6d3f7484",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_df.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ea6a740-1974-4842-bb78-686c3def3817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 documents retrieved which are relevant to the query. Display the first one:\n",
      "\n",
      "---\n",
      "tags:\n",
      "- text-classification\n",
      "- bert\n",
      "---\n",
      "\n",
      "# Model Card for bleurt-tiny-512 \n",
      " \n",
      "# Model Details\n",
      " \n",
      "## Model Description\n",
      " \n",
      "Pytorch version of the original BLEURT models from ACL paper\n",
      " \n",
      "- **Developed by:** Elron Bandel, Thibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research\n",
      "- **Shared by [Optional]:** Elron Bandel\n",
      "- **Model type:** Text Classification \n",
      "- **Language(s) (NLP):** More information needed\n",
      "- **License:** More information needed \n",
      "- **Parent Model:** BERT\n",
      "- **Resources for more information:**\n",
      "     - [GitHub Repo](https://github.com/google-research/bleurt/tree/master)\n",
      " \t  - [Associated Paper](https://aclanthology.org/2020.acl-main.704/)\n",
      "    - [Blog Post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)\n",
      " \t\n",
      "\n",
      "\n",
      "# Uses\n",
      " \n",
      "\n",
      "## Direct Use\n",
      "This model can be used for the task of Text Classification \n",
      " \n",
      "## Downstream Use [Optional]\n",
      " \n",
      "More information needed.\n",
      " \n",
      "## Out-of-Scope Use\n",
      " \n",
      "The model should not be used to intentionally create hostile or alienating environments for people. \n",
      " \n",
      "# Bias, Risks, and Limitations\n",
      " \n",
      " \n",
      "Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n",
      "\n",
      "\n",
      "\n",
      "## Recommendations\n",
      " \n",
      " \n",
      "Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n",
      "\n",
      "# Training Details\n",
      " \n",
      "## Training Data\n",
      "The model authors note in the [associated paper](https://aclanthology.org/2020.acl-main.704.pdf): \n",
      "> We use years 2017 to 2019 of the WMT Metrics Shared Task, to-English language pairs. For each year, we used the of- ficial WMT test set, which include several thou- sand pairs of sentences with human ratings from the news domain. The training sets contain 5,360, 9,492, and 147,691 records for each year. \n",
      " \n",
      " \n",
      "## Training Procedure\n",
      "\n",
      " \n",
      "### Preprocessing\n",
      " \n",
      "More information needed \n",
      " \n",
      "### Speeds, Sizes, Times\n",
      "More information needed \n",
      "\n",
      " \n",
      "# Evaluation\n",
      " \n",
      " \n",
      "## Testing Data, Factors & Metrics\n",
      " \n",
      "### Testing Data\n",
      " \n",
      "The test sets for years 2018 and 2019 [of the WMT Metrics Shared Task, to-English language pairs.]  are noisier,\n",
      " \n",
      " \n",
      " \n",
      "### Factors\n",
      "More information needed\n",
      " \n",
      "### Metrics\n",
      " \n",
      "More information needed\n",
      " \n",
      " \n",
      "## Results \n",
      " \n",
      "More information needed\n",
      "\n",
      " \n",
      "# Model Examination\n",
      " \n",
      "More information needed\n",
      " \n",
      "# Environmental Impact\n",
      " \n",
      "Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n",
      " \n",
      "- **Hardware Type:** More information needed\n",
      "- **Hours used:** More information needed\n",
      "- **Cloud Provider:** More information needed\n",
      "- **Compute Region:** More information needed\n",
      "- **Carbon Emitted:** More information needed\n",
      " \n",
      "# Technical Specifications [optional]\n",
      " \n",
      "## Model Architecture and Objective\n",
      "\n",
      "More information needed \n",
      " \n",
      "## Compute Infrastructure\n",
      " \n",
      "More information needed \n",
      " \n",
      "### Hardware\n",
      " \n",
      " \n",
      "More information needed\n",
      " \n",
      "### Software\n",
      " \n",
      "More information needed.\n",
      " \n",
      "# Citation\n",
      "\n",
      " \n",
      "**BibTeX:**\n",
      " \n",
      " \n",
      "```bibtex\n",
      "@inproceedings{sellam2020bleurt,\n",
      "  title = {BLEURT: Learning Robust Metrics for Text Generation},\n",
      "  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},\n",
      "  year = {2020},\n",
      "  booktitle = {Proceedings of ACL}\n",
      "}\n",
      "```\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "# Glossary [optional]\n",
      "More information needed \n",
      " \n",
      "# More Information [optional]\n",
      "More information needed \n",
      "\n",
      " \n",
      "# Model Card Authors [optional]\n",
      " \n",
      " Elron Bandel in collaboration with Ezi Ozoani and the Hugging Face team\n",
      "\n",
      "\n",
      "# Model Card Contact\n",
      " \n",
      "More information needed\n",
      " \n",
      "# How to Get Started with the Model\n",
      " \n",
      "Use the code below to get started with the model.\n",
      " \n",
      "<details>\n",
      "<summary> Click to expand </summary>\n",
      "\n",
      "```python\n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "import torch\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-tiny-512\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-tiny-512\")\n",
      "model.eval()\n",
      "\n",
      "references = [\"hello world\", \"hello world\"]\n",
      "candidates = [\"hi universe\", \"bye world\"]\n",
      "\n",
      "with torch.no_grad():\n",
      "  scores = model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\n",
      "\n",
      "print(scores) # tensor([-0.9414, -0.5678])\n",
      " ```\n",
      "\n",
      "See [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing) for model conversion code. \n",
      "</details>\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(relevant_documents)} documents retrieved which are relevant to the query. Display the first one:\\n')\n",
    "print(relevant_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715ef2a-fdec-4059-b2b4-61578acbaa7f",
   "metadata": {},
   "source": [
    "# RETRIEVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6fcf38-6348-4f9c-8451-e8decbe5365a",
   "metadata": {},
   "source": [
    "## Create a retriever interface using vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2452b31d-c555-4e51-8247-107608e7f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use similarity searching algorithm and return 3 most relevant documents.\n",
    "# ('similarity', 'similarity_score_threshold', 'mmr')\n",
    "#retriever = vectorstore2.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 5,\"score_threshold\": 0.5})\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3,})\n",
    "retriever2 = vectorstore2.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3,})\n",
    "# \"score_threshold\": 0.5"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c31c85b8-1b85-4f58-b68e-4bb7074915a5",
   "metadata": {},
   "source": [
    "Now we have our vector store and retrieval system ready. \n",
    "We then need a large language model (LLM) to process information and answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "792aab79-979b-4f06-94a3-816c6b60c7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='---\\nlanguage: en\\nlicense: apache-2.0\\ntags:\\n- generated_from_trainer\\nmetrics:\\n- accuracy\\n- f1\\nwidget:\\n- text: The agent on the phone was very helpful and nice to me.\\nbase_model: bert-base-uncased\\nmodel-index:\\n- name: bert-base-uncased-finetuned-surveyclassification\\n  results: []\\n---\\n\\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n# bert-base-uncased-finetuned-surveyclassification\\n\\nThis model is a fine-tuned version of [bert-base-uncased](https://huggingface.co/bert-base-uncased) on a custom survey dataset.\\nIt achieves the following results on the evaluation set:\\n- Loss: 0.2818\\n- Accuracy: 0.9097\\n- F1: 0.9097\\n\\n## Model description\\n\\nMore information needed\\n\\n#### Limitations and bias\\n\\nThis model is limited by its training dataset of survey results for a particular customer service domain. This may not generalize well for all use cases in different domains.\\n\\n#### How to use\\n\\nYou can use this model with Transformers *pipeline* for Text Classification.\\n\\n```python\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\\ntokenizer = AutoTokenizer.from_pretrained(\"Jorgeutd/bert-base-uncased-finetuned-surveyclassification\")\\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Jorgeutd/bert-base-uncased-finetuned-surveyclassification\")\\ntext_classifier = pipeline(\"text-classification\", model=model,tokenizer=tokenizer, device=0)\\nexample = \"The agent on the phone was very helpful and nice to me.\"\\nresults = text_classifier(example)\\nprint(results)\\n```\\n\\n## Training and evaluation data\\n\\nCustom survey dataset.\\n\\n## Training procedure\\nSageMaker notebook instance.\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 3e-05\\n- train_batch_size: 16\\n- eval_batch_size: 16\\n- seed: 42\\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\\n- lr_scheduler_type: linear\\n- lr_scheduler_warmup_steps: 100\\n- num_epochs: 10\\n- mixed_precision_training: Native AMP\\n\\n### Training results\\n\\n| Training Loss | Epoch | Step | Validation Loss | Accuracy | F1     |\\n|:-------------:|:-----:|:----:|:---------------:|:--------:|:------:|\\n| 0.4136        | 1.0   | 902  | 0.2818          | 0.9097   | 0.9097 |\\n| 0.2213        | 2.0   | 1804 | 0.2990          | 0.9077   | 0.9077 |\\n| 0.1548        | 3.0   | 2706 | 0.3507          | 0.9026   | 0.9026 |\\n| 0.1034        | 4.0   | 3608 | 0.4692          | 0.9011   | 0.9011 |\\n\\n\\n### Framework versions\\n\\n- Transformers 4.16.2\\n- Pytorch 1.8.1+cu111\\n- Datasets 1.18.3\\n- Tokenizers 0.11.0', metadata={'source': 'model_cards/Jorgeutd/bert-base-uncased-finetuned-surveyclassification/README.md'})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(query)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f35d7-5ee6-4684-8c5b-b4e3cc715bef",
   "metadata": {},
   "source": [
    "# GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9afce7de-e917-4f8f-87e0-22858b9d7f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the model that can be used for legal document classification? \\nThe model that can be used for legal document classification is a supervised machine learning model, specifically a classification model. The model can be trained on a labeled dataset of legal documents, where each document is labeled with a specific category or class (e.g. contract, agreement, settlement, etc.). The model can then be used to classify new, unseen legal documents into one of the pre-defined categories.\\n\\nSome common classification models used for legal document classification include:\\n\\n1. Naive Bayes (NB)'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=\"\",\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    model_kwargs={\"temperature\":0.1, \"max_length\":500})\n",
    "\n",
    "query = \"\"\"What is the model that can be used for legal document classification?\"\"\" \n",
    "hf.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee5fb89-5124-425c-8e42-4bcefe74b519",
   "metadata": {},
   "source": [
    "##  Open source LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "77a0e6e0-87f9-420a-91b6-515a4bef6b26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.40it/s]\n",
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "login('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')\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    #model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    model_id=\"/root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa\", # refer to local path \n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0.01, \"max_new_tokens\": 300}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ba750ff9-8b0b-4628-af0f-418af9da4abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**\\nA. Naive Bayes\\nB. Decision Trees\\nC. Support Vector Machines\\nD. Random Forest\\nAnswer: C. Support Vector Machines\\nExplanation: Support Vector Machines (SVM) is a popular machine learning model that can be used for legal document classification. SVM is a supervised learning model that can be used for classification and regression tasks. It is particularly effective for high-dimensional data and can handle non-linear relationships between the features and the target variable. SVM has been widely used in various applications, including text classification, image classification, and bioinformatics.\\n\\nIn the context of legal document classification, SVM can be used to classify documents into different categories such as contracts, agreements, lawsuits, and so on. The model can be trained on a dataset of labeled documents, and then used to classify new, unseen documents. SVM is a robust and accurate model that can handle the complexity of legal documents, which often contain complex language and nuanced concepts.\\n\\nThe other options are not as suitable for legal document classification:\\n\\nA. Naive Bayes is a simple probabilistic model that is not as effective for complex tasks like legal document classification.\\n\\nB. Decision Trees are a type of supervised learning model that can be used for classification tasks, but they are not as effective as SVM for high-dimensional data and complex relationships.\\n\\nD. Random Forest is an ensemble learning model that combines multiple decision trees to improve accuracy. While it can be used for classification tasks, it is not as effective as SVM for legal document'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = hf \n",
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268127f-9edc-40ca-a904-662a31ba69eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2285c620-5b0b-4ffb-8ba0-868af471f2be",
   "metadata": {},
   "source": [
    "# Use together the retrieval system for relevant documents and the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4df64ffa-c435-43fe-b289-441f16fafab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
    "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    " template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8547c7f8-a81e-414e-ab2d-5f33f663668c",
   "metadata": {},
   "source": [
    "Call LangChain’s RetrievalQA with the prompt above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8c8e5725-499e-4113-876c-f5350f20ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever2,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5c831-8f8c-40d2-9173-b55efb062ecc",
   "metadata": {},
   "source": [
    "## Use RetrievalQA invoke method to execute the chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62999093-6061-4a5f-9e36-c981779b1c35",
   "metadata": {},
   "source": [
    "### Option 1  (not active for the moment)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6746b49a-6987-4b0d-84b3-15625ca80c88",
   "metadata": {},
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77d84416-c1e1-4ab3-8fb0-aa3a02ace51e",
   "metadata": {},
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain.invoke(\"\"\"Which of the retrieved model cards is the most transparent?\n",
    "Transparency means disclosing more information about the model\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc7d74a-c9be-4e31-9d1a-e2609588aca7",
   "metadata": {},
   "source": [
    "### Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9ee812af-1f9c-4454-86c1-114f3feeae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most transparent model card is the one for the \"bleurt-tiny-512\" model. It provides a detailed description of the model, its training data, and its evaluation metrics. It also includes information about the potential biases and limitations of the model, as well as recommendations for responsible use. Additionally, it provides a clear citation for the model and its associated paper. Overall, the \"bleurt-tiny-512\" model card is the most transparent because it provides a comprehensive overview of the model and its capabilities.\n"
     ]
    }
   ],
   "source": [
    "# Call the QA chain with our new query regarding the transparency\n",
    "query = \"\"\"Which of the retrieved model cards is the most transparent?\n",
    "Transparency means disclosing more information about the model\"\"\"\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0555f584-e064-4492-b898-d2625425e8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>llm</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>relevant_documents</th>\n",
       "      <th>datetime</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\ntags:\\n- text-classificatio...</td>\n",
       "      <td>2024-06-19 14:29:28.161965</td>\n",
       "      <td>model_cards/Elron/bleurt-tiny-512/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\ninference: false\\nlicense: ...</td>\n",
       "      <td>2024-06-19 14:29:28.161965</td>\n",
       "      <td>model_cards/Fujitsu/AugCode/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\nlanguage: en\\nlicense: apac...</td>\n",
       "      <td>2024-06-19 14:29:28.161965</td>\n",
       "      <td>model_cards/Hate-speech-CNERG/bert-base-uncase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>page_content='---\\nlanguage:\\n- da\\nlicense: a...</td>\n",
       "      <td>2024-06-19 14:29:28.161965</td>\n",
       "      <td>model_cards/alexandrainst/da-ned-base/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\ntags:\\n- text-classificatio...</td>\n",
       "      <td>2024-06-19 15:17:06.590810</td>\n",
       "      <td>model_cards/Elron/bleurt-tiny-512/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\ninference: false\\nlicense: ...</td>\n",
       "      <td>2024-06-19 15:17:06.590810</td>\n",
       "      <td>model_cards/Fujitsu/AugCode/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlanguage: en\\nlicense: apac...</td>\n",
       "      <td>2024-06-19 15:17:06.590810</td>\n",
       "      <td>model_cards/Hate-speech-CNERG/bert-base-uncase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\ntags:\\n- text-classificatio...</td>\n",
       "      <td>2024-06-19 15:45:39.190186</td>\n",
       "      <td>model_cards/Elron/bleurt-tiny-512/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\ninference: false\\nlicense: ...</td>\n",
       "      <td>2024-06-19 15:45:39.190186</td>\n",
       "      <td>model_cards/Fujitsu/AugCode/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlanguage: en\\nlicense: apac...</td>\n",
       "      <td>2024-06-19 15:45:39.190186</td>\n",
       "      <td>model_cards/Hate-speech-CNERG/bert-base-uncase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlanguage:\\n- da\\nlicense: a...</td>\n",
       "      <td>2024-06-19 15:45:39.190186</td>\n",
       "      <td>model_cards/alexandrainst/da-ned-base/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\ntags:\\n- text-classificatio...</td>\n",
       "      <td>2024-06-19 15:51:18.960444</td>\n",
       "      <td>model_cards/Elron/bleurt-tiny-512/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\ninference: false\\nlicense: ...</td>\n",
       "      <td>2024-06-19 15:51:18.960444</td>\n",
       "      <td>model_cards/Fujitsu/AugCode/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the model that can be used for legal d...</td>\n",
       "      <td>The model that can be used for legal document ...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlanguage: en\\nlicense: apac...</td>\n",
       "      <td>2024-06-19 15:51:18.960444</td>\n",
       "      <td>model_cards/Hate-speech-CNERG/bert-base-uncase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Which of the retrieved model cards is the most...</td>\n",
       "      <td>The most transparent model card is the first o...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlanguage: en\\nlicense: apac...</td>\n",
       "      <td>2024-06-20 09:24:47.262861</td>\n",
       "      <td>model_cards/Jorgeutd/bert-base-uncased-finetun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Which of the retrieved model cards is the most...</td>\n",
       "      <td>The most transparent model card is the first o...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlicense: apache-2.0\\ntags:\\...</td>\n",
       "      <td>2024-06-20 09:24:47.262861</td>\n",
       "      <td>model_cards/ASCCCCCCCC/distilbert-base-uncased...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Which of the retrieved model cards is the most...</td>\n",
       "      <td>The most transparent model card is the first o...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlicense: apache-2.0\\ntags:\\...</td>\n",
       "      <td>2024-06-20 09:24:47.262861</td>\n",
       "      <td>model_cards/ASCCCCCCCC/PENGMENGJIE-finetuned-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Which of the retrieved model cards is the most...</td>\n",
       "      <td>The most transparent model card is the first o...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlanguage: en\\nlicense: apac...</td>\n",
       "      <td>2024-06-20 09:29:11.729504</td>\n",
       "      <td>model_cards/Jorgeutd/bert-base-uncased-finetun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Which of the retrieved model cards is the most...</td>\n",
       "      <td>The most transparent model card is the first o...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlicense: apache-2.0\\ntags:\\...</td>\n",
       "      <td>2024-06-20 09:29:11.729504</td>\n",
       "      <td>model_cards/ASCCCCCCCC/distilbert-base-uncased...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Which of the retrieved model cards is the most...</td>\n",
       "      <td>The most transparent model card is the first o...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlicense: apache-2.0\\ntags:\\...</td>\n",
       "      <td>2024-06-20 09:29:11.729504</td>\n",
       "      <td>model_cards/ASCCCCCCCC/PENGMENGJIE-finetuned-e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Which of the retrieved model cards is the most...</td>\n",
       "      <td>The most transparent model card is the one for...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\ntags:\\n- text-classificatio...</td>\n",
       "      <td>2024-06-20 09:37:23.804637</td>\n",
       "      <td>model_cards/Elron/bleurt-tiny-512/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Which of the retrieved model cards is the most...</td>\n",
       "      <td>The most transparent model card is the one for...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlanguage:\\n- da\\nlicense: a...</td>\n",
       "      <td>2024-06-20 09:37:23.804637</td>\n",
       "      <td>model_cards/alexandrainst/da-ned-base/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Which of the retrieved model cards is the most...</td>\n",
       "      <td>The most transparent model card is the one for...</td>\n",
       "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>page_content='---\\nlanguage: en\\nlicense: apac...</td>\n",
       "      <td>2024-06-20 09:37:23.804637</td>\n",
       "      <td>model_cards/Hate-speech-CNERG/bert-base-uncase...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               prompt  \\\n",
       "0   What is the model that can be used for legal d...   \n",
       "1   What is the model that can be used for legal d...   \n",
       "2   What is the model that can be used for legal d...   \n",
       "3   What is the model that can be used for legal d...   \n",
       "4   What is the model that can be used for legal d...   \n",
       "5   What is the model that can be used for legal d...   \n",
       "6   What is the model that can be used for legal d...   \n",
       "7   What is the model that can be used for legal d...   \n",
       "8   What is the model that can be used for legal d...   \n",
       "9   What is the model that can be used for legal d...   \n",
       "10  What is the model that can be used for legal d...   \n",
       "11  What is the model that can be used for legal d...   \n",
       "12  What is the model that can be used for legal d...   \n",
       "13  What is the model that can be used for legal d...   \n",
       "14  Which of the retrieved model cards is the most...   \n",
       "15  Which of the retrieved model cards is the most...   \n",
       "16  Which of the retrieved model cards is the most...   \n",
       "17  Which of the retrieved model cards is the most...   \n",
       "18  Which of the retrieved model cards is the most...   \n",
       "19  Which of the retrieved model cards is the most...   \n",
       "20  Which of the retrieved model cards is the most...   \n",
       "21  Which of the retrieved model cards is the most...   \n",
       "22  Which of the retrieved model cards is the most...   \n",
       "\n",
       "                                             response  \\\n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                       \n",
       "3                                                       \n",
       "4   The model that can be used for legal document ...   \n",
       "5   The model that can be used for legal document ...   \n",
       "6   The model that can be used for legal document ...   \n",
       "7   The model that can be used for legal document ...   \n",
       "8   The model that can be used for legal document ...   \n",
       "9   The model that can be used for legal document ...   \n",
       "10  The model that can be used for legal document ...   \n",
       "11  The model that can be used for legal document ...   \n",
       "12  The model that can be used for legal document ...   \n",
       "13  The model that can be used for legal document ...   \n",
       "14  The most transparent model card is the first o...   \n",
       "15  The most transparent model card is the first o...   \n",
       "16  The most transparent model card is the first o...   \n",
       "17  The most transparent model card is the first o...   \n",
       "18  The most transparent model card is the first o...   \n",
       "19  The most transparent model card is the first o...   \n",
       "20  The most transparent model card is the one for...   \n",
       "21  The most transparent model card is the one for...   \n",
       "22  The most transparent model card is the one for...   \n",
       "\n",
       "                                    llm         embedding_model  \\\n",
       "0                                                                 \n",
       "1                                                                 \n",
       "2                                                                 \n",
       "3                                                                 \n",
       "4   meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "5   meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "6   meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "7   meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "8   meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "9   meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "10  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "11  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "12  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "13  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "14  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "15  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "16  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "17  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "18  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "19  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "20  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "21  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "22  meta-llama/Meta-Llama-3-8B-Instruct  BAAI/bge-small-en-v1.5   \n",
       "\n",
       "                                   relevant_documents  \\\n",
       "0   page_content='---\\ntags:\\n- text-classificatio...   \n",
       "1   page_content='---\\ninference: false\\nlicense: ...   \n",
       "2   page_content='---\\nlanguage: en\\nlicense: apac...   \n",
       "3   page_content='---\\nlanguage:\\n- da\\nlicense: a...   \n",
       "4   page_content='---\\ntags:\\n- text-classificatio...   \n",
       "5   page_content='---\\ninference: false\\nlicense: ...   \n",
       "6   page_content='---\\nlanguage: en\\nlicense: apac...   \n",
       "7   page_content='---\\ntags:\\n- text-classificatio...   \n",
       "8   page_content='---\\ninference: false\\nlicense: ...   \n",
       "9   page_content='---\\nlanguage: en\\nlicense: apac...   \n",
       "10  page_content='---\\nlanguage:\\n- da\\nlicense: a...   \n",
       "11  page_content='---\\ntags:\\n- text-classificatio...   \n",
       "12  page_content='---\\ninference: false\\nlicense: ...   \n",
       "13  page_content='---\\nlanguage: en\\nlicense: apac...   \n",
       "14  page_content='---\\nlanguage: en\\nlicense: apac...   \n",
       "15  page_content='---\\nlicense: apache-2.0\\ntags:\\...   \n",
       "16  page_content='---\\nlicense: apache-2.0\\ntags:\\...   \n",
       "17  page_content='---\\nlanguage: en\\nlicense: apac...   \n",
       "18  page_content='---\\nlicense: apache-2.0\\ntags:\\...   \n",
       "19  page_content='---\\nlicense: apache-2.0\\ntags:\\...   \n",
       "20  page_content='---\\ntags:\\n- text-classificatio...   \n",
       "21  page_content='---\\nlanguage:\\n- da\\nlicense: a...   \n",
       "22  page_content='---\\nlanguage: en\\nlicense: apac...   \n",
       "\n",
       "                     datetime  \\\n",
       "0  2024-06-19 14:29:28.161965   \n",
       "1  2024-06-19 14:29:28.161965   \n",
       "2  2024-06-19 14:29:28.161965   \n",
       "3  2024-06-19 14:29:28.161965   \n",
       "4  2024-06-19 15:17:06.590810   \n",
       "5  2024-06-19 15:17:06.590810   \n",
       "6  2024-06-19 15:17:06.590810   \n",
       "7  2024-06-19 15:45:39.190186   \n",
       "8  2024-06-19 15:45:39.190186   \n",
       "9  2024-06-19 15:45:39.190186   \n",
       "10 2024-06-19 15:45:39.190186   \n",
       "11 2024-06-19 15:51:18.960444   \n",
       "12 2024-06-19 15:51:18.960444   \n",
       "13 2024-06-19 15:51:18.960444   \n",
       "14 2024-06-20 09:24:47.262861   \n",
       "15 2024-06-20 09:24:47.262861   \n",
       "16 2024-06-20 09:24:47.262861   \n",
       "17 2024-06-20 09:29:11.729504   \n",
       "18 2024-06-20 09:29:11.729504   \n",
       "19 2024-06-20 09:29:11.729504   \n",
       "20 2024-06-20 09:37:23.804637   \n",
       "21 2024-06-20 09:37:23.804637   \n",
       "22 2024-06-20 09:37:23.804637   \n",
       "\n",
       "                                               source  \n",
       "0         model_cards/Elron/bleurt-tiny-512/README.md  \n",
       "1               model_cards/Fujitsu/AugCode/README.md  \n",
       "2   model_cards/Hate-speech-CNERG/bert-base-uncase...  \n",
       "3     model_cards/alexandrainst/da-ned-base/README.md  \n",
       "4         model_cards/Elron/bleurt-tiny-512/README.md  \n",
       "5               model_cards/Fujitsu/AugCode/README.md  \n",
       "6   model_cards/Hate-speech-CNERG/bert-base-uncase...  \n",
       "7         model_cards/Elron/bleurt-tiny-512/README.md  \n",
       "8               model_cards/Fujitsu/AugCode/README.md  \n",
       "9   model_cards/Hate-speech-CNERG/bert-base-uncase...  \n",
       "10    model_cards/alexandrainst/da-ned-base/README.md  \n",
       "11        model_cards/Elron/bleurt-tiny-512/README.md  \n",
       "12              model_cards/Fujitsu/AugCode/README.md  \n",
       "13  model_cards/Hate-speech-CNERG/bert-base-uncase...  \n",
       "14  model_cards/Jorgeutd/bert-base-uncased-finetun...  \n",
       "15  model_cards/ASCCCCCCCC/distilbert-base-uncased...  \n",
       "16  model_cards/ASCCCCCCCC/PENGMENGJIE-finetuned-e...  \n",
       "17  model_cards/Jorgeutd/bert-base-uncased-finetun...  \n",
       "18  model_cards/ASCCCCCCCC/distilbert-base-uncased...  \n",
       "19  model_cards/ASCCCCCCCC/PENGMENGJIE-finetuned-e...  \n",
       "20        model_cards/Elron/bleurt-tiny-512/README.md  \n",
       "21    model_cards/alexandrainst/da-ned-base/README.md  \n",
       "22  model_cards/Hate-speech-CNERG/bert-base-uncase...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = datetime.now()\n",
    "for doc in result['source_documents']:\n",
    "    prompts_df.loc[len(prompts_df)] = {\n",
    "        \"prompt\": query, \n",
    "        \"response\": result['result'], \n",
    "        \"llm\": \"meta-llama/Meta-Llama-3-8B-Instruct\", \n",
    "        \"embedding_model\": \"BAAI/bge-small-en-v1.5\",\n",
    "        \"relevant_documents\": str(doc), \n",
    "        \"datetime\": date,\n",
    "        \"source\": str(doc.metadata[\"source\"])}\n",
    "prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7a96c139-c71f-4182-8b4b-89755f6d6bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Which of the retrieved model cards is the most transparent?\\nTransparency means disclosing more information about the model',\n",
       " 'response': 'The most transparent model card is the one for the \"bleurt-tiny-512\" model. It provides a detailed description of the model, its training data, and its evaluation metrics. It also includes information about the potential biases and limitations of the model, as well as recommendations for responsible use. Additionally, it provides a clear citation for the model and its associated paper. Overall, the \"bleurt-tiny-512\" model card is the most transparent because it provides a comprehensive overview of the model and its capabilities.',\n",
       " 'llm': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       " 'embedding_model': 'BAAI/bge-small-en-v1.5',\n",
       " 'relevant_documents': 'page_content=\\'---\\\\nlanguage: en\\\\nlicense: apache-2.0\\\\ndatasets:\\\\n  - hatexplain\\\\n---\\\\n\\\\n\\\\n## Table of Contents\\\\n- [Model Details](#model-details)\\\\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\\\\n- [Uses](#uses)\\\\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\\\\n- [Training](#training)\\\\n- [Evaluation](#evaluation)\\\\n- [Technical Specifications](#technical-specifications)\\\\n- [Citation Information](#citation-information)\\\\n\\\\n## Model Details\\\\n**Model Description:** \\\\nThe model is used for classifying a text as Abusive (Hatespeech and Offensive) or Normal. The model is trained using data from Gab and Twitter and Human Rationales were included as part of the training data to boost the performance. The model also has a rationale predictor head that can predict the rationales given an abusive sentence\\\\n\\\\n\\\\n- **Developed by:** Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee \\\\n- **Model Type:** Text Classification\\\\n- **Language(s):**  English\\\\n- **License:**  Apache-2.0\\\\n- **Parent Model:** See the [BERT base uncased model](https://huggingface.co/bert-base-uncased) for more information about the BERT base model.\\\\n- **Resources for more information:**\\\\n  - [Research Paper](https://arxiv.org/abs/2012.10289) Accepted at AAAI 2021.\\\\n  - [GitHub Repo with datatsets and models](https://github.com/punyajoy/HateXplain)\\\\n\\\\n\\\\n                                                                                               \\\\n## How to Get Started with the Model\\\\n\\\\n**Details of usage**\\\\n\\\\nPlease use the **Model_Rational_Label** class inside [models.py](models.py) to load the models. The default prediction in this hosted inference API may be wrong due to the use of different class initialisations.\\\\n\\\\n```python\\\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\\\n### from models.py\\\\nfrom models import *\\\\ntokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\\\\nmodel = Model_Rational_Label.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\\\\ninputs = tokenizer(\\\\\\'He is a great guy\", return_tensors=\"pt\")\\\\nprediction_logits, _ = model(input_ids=inputs[\\\\\\'input_ids\\\\\\'],attention_mask=inputs[\\\\\\'attention_mask\\\\\\'])\\\\n```\\\\n\\\\n## Uses\\\\n\\\\n#### Direct Use\\\\n\\\\nThis model can be used for Text Classification\\\\n\\\\n\\\\n#### Downstream Use\\\\n\\\\n[More information needed]\\\\n\\\\n#### Misuse and Out-of-scope Use\\\\n\\\\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\\\\n\\\\n## Risks, Limitations and Biases\\\\n\\\\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\\\\n\\\\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).\\\\n\\\\n(and if you can generate an example of a biased prediction, also something like this): \\\\n\\\\nPredictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For ![example:](https://github.com/hate-alert/HateXplain/blob/master/Figures/dataset_example.png) \\\\n\\\\nThe model author\\\\\\'s also note in their HateXplain paper that they \\\\n>  *have not considered any external context such as profile bio, user gender, history of posts etc., which might be helpful in the classification task. Also, in this work we have focused on the English language. It does not consider multilingual hate speech into account.*\\\\n\\\\n\\\\n#### Training Procedure\\\\n\\\\n##### Preprocessing\\\\n\\\\nThe authors detail their preprocessing procedure in the [Github repository](https://github.com/hate-alert/HateXplain/tree/master/Preprocess)\\\\n\\\\n\\\\n## Evaluation\\\\nThe mode authors detail the Hidden layer size and attention for the HateXplain fien tuned models in the [associated paper](https://arxiv.org/pdf/2012.10289.pdf)\\\\n\\\\n#### Results \\\\n\\\\nThe model authors both in their paper and in the git repository provide the illustrative output of the BERT - HateXplain in comparison to BERT and and other HateXplain fine tuned ![models]( https://github.com/hate-alert/HateXplain/blob/master/Figures/bias-subgroup.pdf)\\\\n\\\\n## Citation Information\\\\n\\\\n```bibtex\\\\n@article{mathew2020hatexplain,\\\\n  title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},\\\\n  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},\\\\n  journal={arXiv preprint arXiv:2012.10289},\\\\n  year={2020}\\\\n\\\\n}\\\\n```\\' metadata={\\'source\\': \\'model_cards/Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two/README.md\\'}',\n",
       " 'datetime': Timestamp('2024-06-20 09:37:23.804637'),\n",
       " 'source': 'model_cards/Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two/README.md'}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_df.iloc[22].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ee7b312e-329a-4d19-bfc7-e058a122d6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_df.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b0056d3c-513f-4562-97f9-5b087c0a82b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='---\\ntags:\\n- text-classification\\n- bert\\n---\\n\\n# Model Card for bleurt-tiny-512 \\n \\n# Model Details\\n \\n## Model Description\\n \\nPytorch version of the original BLEURT models from ACL paper\\n \\n- **Developed by:** Elron Bandel, Thibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research\\n- **Shared by [Optional]:** Elron Bandel\\n- **Model type:** Text Classification \\n- **Language(s) (NLP):** More information needed\\n- **License:** More information needed \\n- **Parent Model:** BERT\\n- **Resources for more information:**\\n     - [GitHub Repo](https://github.com/google-research/bleurt/tree/master)\\n \\t  - [Associated Paper](https://aclanthology.org/2020.acl-main.704/)\\n    - [Blog Post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)\\n \\t\\n\\n\\n# Uses\\n \\n\\n## Direct Use\\nThis model can be used for the task of Text Classification \\n \\n## Downstream Use [Optional]\\n \\nMore information needed.\\n \\n## Out-of-Scope Use\\n \\nThe model should not be used to intentionally create hostile or alienating environments for people. \\n \\n# Bias, Risks, and Limitations\\n \\n \\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\\n\\n\\n\\n## Recommendations\\n \\n \\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\\n\\n# Training Details\\n \\n## Training Data\\nThe model authors note in the [associated paper](https://aclanthology.org/2020.acl-main.704.pdf): \\n> We use years 2017 to 2019 of the WMT Metrics Shared Task, to-English language pairs. For each year, we used the of- ficial WMT test set, which include several thou- sand pairs of sentences with human ratings from the news domain. The training sets contain 5,360, 9,492, and 147,691 records for each year. \\n \\n \\n## Training Procedure\\n\\n \\n### Preprocessing\\n \\nMore information needed \\n \\n### Speeds, Sizes, Times\\nMore information needed \\n\\n \\n# Evaluation\\n \\n \\n## Testing Data, Factors & Metrics\\n \\n### Testing Data\\n \\nThe test sets for years 2018 and 2019 [of the WMT Metrics Shared Task, to-English language pairs.]  are noisier,\\n \\n \\n \\n### Factors\\nMore information needed\\n \\n### Metrics\\n \\nMore information needed\\n \\n \\n## Results \\n \\nMore information needed\\n\\n \\n# Model Examination\\n \\nMore information needed\\n \\n# Environmental Impact\\n \\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\\n \\n- **Hardware Type:** More information needed\\n- **Hours used:** More information needed\\n- **Cloud Provider:** More information needed\\n- **Compute Region:** More information needed\\n- **Carbon Emitted:** More information needed\\n \\n# Technical Specifications [optional]\\n \\n## Model Architecture and Objective\\n\\nMore information needed \\n \\n## Compute Infrastructure\\n \\nMore information needed \\n \\n### Hardware\\n \\n \\nMore information needed\\n \\n### Software\\n \\nMore information needed.\\n \\n# Citation\\n\\n \\n**BibTeX:**\\n \\n \\n```bibtex\\n@inproceedings{sellam2020bleurt,\\n  title = {BLEURT: Learning Robust Metrics for Text Generation},\\n  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},\\n  year = {2020},\\n  booktitle = {Proceedings of ACL}\\n}\\n```\\n \\n \\n \\n \\n# Glossary [optional]\\nMore information needed \\n \\n# More Information [optional]\\nMore information needed \\n\\n \\n# Model Card Authors [optional]\\n \\n Elron Bandel in collaboration with Ezi Ozoani and the Hugging Face team\\n\\n\\n# Model Card Contact\\n \\nMore information needed\\n \\n# How to Get Started with the Model\\n \\nUse the code below to get started with the model.\\n \\n<details>\\n<summary> Click to expand </summary>\\n\\n```python\\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\\nimport torch\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-tiny-512\")\\nmodel = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-tiny-512\")\\nmodel.eval()\\n\\nreferences = [\"hello world\", \"hello world\"]\\ncandidates = [\"hi universe\", \"bye world\"]\\n\\nwith torch.no_grad():\\n  scores = model(**tokenizer(references, candidates, return_tensors=\\'pt\\'))[0].squeeze()\\n\\nprint(scores) # tensor([-0.9414, -0.5678])\\n ```\\n\\nSee [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing) for model conversion code. \\n</details>', metadata={'source': 'model_cards/Elron/bleurt-tiny-512/README.md'}), Document(page_content='---\\nlanguage:\\n- da\\nlicense: apache-2.0\\n---\\n\\n# XLM-Roberta fine-tuned for Named Entity Disambiguation\\n\\nGiven a sentence and a knowledge graph context, the model detects whether a specific entity (represented by the knowledge graph context) is mentioned in the sentence (binary classification). \\nThe base language model used is the [xlm-roberta-base](https://huggingface.co/xlm-roberta-base). \\n\\nHere is how to use the model: \\n\\n```python\\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\\n\\nmodel = XLMRobertaForSequenceClassification.from_pretrained(\"alexandrainst/da-ned-base\")\\ntokenizer = XLMRobertaTokenizer.from_pretrained(\"alexandrainst/da-ned-base\")\\n```\\n\\nThe tokenizer takes 2 strings has input: the sentence and the knowledge graph (KG) context. \\nHere is an example:\\n```python\\nsentence = \"Karen Blixen vendte tilbage til Danmark, hvor hun boede resten af sit liv på Rungstedlund, som hun arvede efter sin mor i 1939\"\\nkg_context = \"udmærkelser modtaget Kritikerprisen udmærkelser modtaget Tagea Brandts Rejselegat udmærkelser modtaget Ingenio et arti udmærkelser modtaget Holbergmedaljen udmærkelser modtaget De Gyldne Laurbær mor Ingeborg Dinesen ægtefælle Bror von Blixen-Finecke køn kvinde Commons-kategori Karen Blixen LCAuth no95003722 VIAF 90663542 VIAF 121643918 GND-identifikator 118637878 ISNI 0000 0001 2096 6265 ISNI 0000 0003 6863 4408 ISNI 0000 0001 1891 0457 fødested Rungstedlund fødested Rungsted dødssted Rungstedlund dødssted København statsborgerskab Danmark NDL-nummer 00433530 dødsdato +1962-09-07T00:00:00Z dødsdato +1962-01-01T00:00:00Z fødselsdato +1885-04-17T00:00:00Z fødselsdato +1885-01-01T00:00:00Z AUT NKC jn20000600905 AUT NKC jo2015880827 AUT NKC xx0196181 emnets hovedkategori Kategori:Karen Blixen tilfælde af menneske billede Karen Blixen cropped from larger original.jpg IMDb-identifikationsnummer nm0227598 Freebase-ID /m/04ymd8w BNF 118857710 beskæftigelse skribent beskæftigelse selvbiograf beskæftigelse novelleforfatter ...\"\\n```\\n\\nA KG context, for a specific entity, can be generated from its Wikidata page. \\nIn the previous example, the KG context is a string representation of the Wikidata page of [Karen Blixen (QID=Q182804)](https://www.wikidata.org/wiki/Q182804). \\nSee the [DaNLP documentation](https://danlp-alexandra.readthedocs.io/en/latest/docs/tasks/ned.html#xlmr) for more details about how to generate a KG context. \\n\\n\\n## Training Data \\n\\nThe model has been trained on the [DaNED](https://danlp-alexandra.readthedocs.io/en/latest/docs/datasets.html#daned) and [DaWikiNED](https://danlp-alexandra.readthedocs.io/en/latest/docs/datasets.html#dawikined) datasets.', metadata={'source': 'model_cards/alexandrainst/da-ned-base/README.md'}), Document(page_content='---\\nlanguage: en\\nlicense: apache-2.0\\ndatasets:\\n  - hatexplain\\n---\\n\\n\\n## Table of Contents\\n- [Model Details](#model-details)\\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\\n- [Uses](#uses)\\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\\n- [Training](#training)\\n- [Evaluation](#evaluation)\\n- [Technical Specifications](#technical-specifications)\\n- [Citation Information](#citation-information)\\n\\n## Model Details\\n**Model Description:** \\nThe model is used for classifying a text as Abusive (Hatespeech and Offensive) or Normal. The model is trained using data from Gab and Twitter and Human Rationales were included as part of the training data to boost the performance. The model also has a rationale predictor head that can predict the rationales given an abusive sentence\\n\\n\\n- **Developed by:** Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee \\n- **Model Type:** Text Classification\\n- **Language(s):**  English\\n- **License:**  Apache-2.0\\n- **Parent Model:** See the [BERT base uncased model](https://huggingface.co/bert-base-uncased) for more information about the BERT base model.\\n- **Resources for more information:**\\n  - [Research Paper](https://arxiv.org/abs/2012.10289) Accepted at AAAI 2021.\\n  - [GitHub Repo with datatsets and models](https://github.com/punyajoy/HateXplain)\\n\\n\\n                                                                                               \\n## How to Get Started with the Model\\n\\n**Details of usage**\\n\\nPlease use the **Model_Rational_Label** class inside [models.py](models.py) to load the models. The default prediction in this hosted inference API may be wrong due to the use of different class initialisations.\\n\\n```python\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\\n### from models.py\\nfrom models import *\\ntokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\\nmodel = Model_Rational_Label.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\\ninputs = tokenizer(\\'He is a great guy\", return_tensors=\"pt\")\\nprediction_logits, _ = model(input_ids=inputs[\\'input_ids\\'],attention_mask=inputs[\\'attention_mask\\'])\\n```\\n\\n## Uses\\n\\n#### Direct Use\\n\\nThis model can be used for Text Classification\\n\\n\\n#### Downstream Use\\n\\n[More information needed]\\n\\n#### Misuse and Out-of-scope Use\\n\\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\\n\\n## Risks, Limitations and Biases\\n\\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\\n\\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).\\n\\n(and if you can generate an example of a biased prediction, also something like this): \\n\\nPredictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For ![example:](https://github.com/hate-alert/HateXplain/blob/master/Figures/dataset_example.png) \\n\\nThe model author\\'s also note in their HateXplain paper that they \\n>  *have not considered any external context such as profile bio, user gender, history of posts etc., which might be helpful in the classification task. Also, in this work we have focused on the English language. It does not consider multilingual hate speech into account.*\\n\\n\\n#### Training Procedure\\n\\n##### Preprocessing\\n\\nThe authors detail their preprocessing procedure in the [Github repository](https://github.com/hate-alert/HateXplain/tree/master/Preprocess)\\n\\n\\n## Evaluation\\nThe mode authors detail the Hidden layer size and attention for the HateXplain fien tuned models in the [associated paper](https://arxiv.org/pdf/2012.10289.pdf)\\n\\n#### Results \\n\\nThe model authors both in their paper and in the git repository provide the illustrative output of the BERT - HateXplain in comparison to BERT and and other HateXplain fine tuned ![models]( https://github.com/hate-alert/HateXplain/blob/master/Figures/bias-subgroup.pdf)\\n\\n## Citation Information\\n\\n```bibtex\\n@article{mathew2020hatexplain,\\n  title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},\\n  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},\\n  journal={arXiv preprint arXiv:2012.10289},\\n  year={2020}\\n\\n}\\n```', metadata={'source': 'model_cards/Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two/README.md'})]\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = result['source_documents']\n",
    "print(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3e86c4bf-ae69-4d10-8f49-af0e47068e1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 documents retrieved which are relevant to the query.\n",
      "****************************************************************************************************\n",
      "Relevant Document #1:\n",
      "Source file: model_cards/Elron/bleurt-tiny-512/README.md, \n",
      "Content: ---\n",
      "tags:\n",
      "- text-classification\n",
      "- bert\n",
      "---\n",
      "\n",
      "# Model Card for bleurt-tiny-512 \n",
      " \n",
      "# Model Details\n",
      " \n",
      "## Model Description\n",
      " \n",
      "Pytorch version of the original BLEURT models from ACL paper\n",
      " \n",
      "- **Developed by:** Elron Bandel, Thibault Sellam, Dipanjan Das and Ankur P. Parikh of Google Research\n",
      "- **Shared by [Optional]:** Elron Bandel\n",
      "- **Model type:** Text Classification \n",
      "- **Language(s) (NLP):** More information needed\n",
      "- **License:** More information needed \n",
      "- **Parent Model:** BERT\n",
      "- **Resources for more information:**\n",
      "     - [GitHub Repo](https://github.com/google-research/bleurt/tree/master)\n",
      " \t  - [Associated Paper](https://aclanthology.org/2020.acl-main.704/)\n",
      "    - [Blog Post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html)\n",
      " \t\n",
      "\n",
      "\n",
      "# Uses\n",
      " \n",
      "\n",
      "## Direct Use\n",
      "This model can be used for the task of Text Classification \n",
      " \n",
      "## Downstream Use [Optional]\n",
      " \n",
      "More information needed.\n",
      " \n",
      "## Out-of-Scope Use\n",
      " \n",
      "The model should not be used to intentionally create hostile or alienating environments for people. \n",
      " \n",
      "# Bias, Risks, and Limitations\n",
      " \n",
      " \n",
      "Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.\n",
      "\n",
      "\n",
      "\n",
      "## Recommendations\n",
      " \n",
      " \n",
      "Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n",
      "\n",
      "# Training Details\n",
      " \n",
      "## Training Data\n",
      "The model authors note in the [associated paper](https://aclanthology.org/2020.acl-main.704.pdf): \n",
      "> We use years 2017 to 2019 of the WMT Metrics Shared Task, to-English language pairs. For each year, we used the of- ficial WMT test set, which include several thou- sand pairs of sentences with human ratings from the news domain. The training sets contain 5,360, 9,492, and 147,691 records for each year. \n",
      " \n",
      " \n",
      "## Training Procedure\n",
      "\n",
      " \n",
      "### Preprocessing\n",
      " \n",
      "More information needed \n",
      " \n",
      "### Speeds, Sizes, Times\n",
      "More information needed \n",
      "\n",
      " \n",
      "# Evaluation\n",
      " \n",
      " \n",
      "## Testing Data, Factors & Metrics\n",
      " \n",
      "### Testing Data\n",
      " \n",
      "The test sets for years 2018 and 2019 [of the WMT Metrics Shared Task, to-English language pairs.]  are noisier,\n",
      " \n",
      " \n",
      " \n",
      "### Factors\n",
      "More information needed\n",
      " \n",
      "### Metrics\n",
      " \n",
      "More information needed\n",
      " \n",
      " \n",
      "## Results \n",
      " \n",
      "More information needed\n",
      "\n",
      " \n",
      "# Model Examination\n",
      " \n",
      "More information needed\n",
      " \n",
      "# Environmental Impact\n",
      " \n",
      "Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n",
      " \n",
      "- **Hardware Type:** More information needed\n",
      "- **Hours used:** More information needed\n",
      "- **Cloud Provider:** More information needed\n",
      "- **Compute Region:** More information needed\n",
      "- **Carbon Emitted:** More information needed\n",
      " \n",
      "# Technical Specifications [optional]\n",
      " \n",
      "## Model Architecture and Objective\n",
      "\n",
      "More information needed \n",
      " \n",
      "## Compute Infrastructure\n",
      " \n",
      "More information needed \n",
      " \n",
      "### Hardware\n",
      " \n",
      " \n",
      "More information needed\n",
      " \n",
      "### Software\n",
      " \n",
      "More information needed.\n",
      " \n",
      "# Citation\n",
      "\n",
      " \n",
      "**BibTeX:**\n",
      " \n",
      " \n",
      "```bibtex\n",
      "@inproceedings{sellam2020bleurt,\n",
      "  title = {BLEURT: Learning Robust Metrics for Text Generation},\n",
      "  author = {Thibault Sellam and Dipanjan Das and Ankur P Parikh},\n",
      "  year = {2020},\n",
      "  booktitle = {Proceedings of ACL}\n",
      "}\n",
      "```\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "# Glossary [optional]\n",
      "More information needed \n",
      " \n",
      "# More Information [optional]\n",
      "More information needed \n",
      "\n",
      " \n",
      "# Model Card Authors [optional]\n",
      " \n",
      " Elron Bandel in collaboration with Ezi Ozoani and the Hugging Face team\n",
      "\n",
      "\n",
      "# Model Card Contact\n",
      " \n",
      "More information needed\n",
      " \n",
      "# How to Get Started with the Model\n",
      " \n",
      "Use the code below to get started with the model.\n",
      " \n",
      "<details>\n",
      "<summary> Click to expand </summary>\n",
      "\n",
      "```python\n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "import torch\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-tiny-512\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-tiny-512\")\n",
      "model.eval()\n",
      "\n",
      "references = [\"hello world\", \"hello world\"]\n",
      "candidates = [\"hi universe\", \"bye world\"]\n",
      "\n",
      "with torch.no_grad():\n",
      "  scores = model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\n",
      "\n",
      "print(scores) # tensor([-0.9414, -0.5678])\n",
      " ```\n",
      "\n",
      "See [this notebook](https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing) for model conversion code. \n",
      "</details>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 3 documents retrieved which are relevant to the query.\n",
      "Relevant Document #2:\n",
      "Source file: model_cards/alexandrainst/da-ned-base/README.md, \n",
      "Content: ---\n",
      "language:\n",
      "- da\n",
      "license: apache-2.0\n",
      "---\n",
      "\n",
      "# XLM-Roberta fine-tuned for Named Entity Disambiguation\n",
      "\n",
      "Given a sentence and a knowledge graph context, the model detects whether a specific entity (represented by the knowledge graph context) is mentioned in the sentence (binary classification). \n",
      "The base language model used is the [xlm-roberta-base](https://huggingface.co/xlm-roberta-base). \n",
      "\n",
      "Here is how to use the model: \n",
      "\n",
      "```python\n",
      "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
      "\n",
      "model = XLMRobertaForSequenceClassification.from_pretrained(\"alexandrainst/da-ned-base\")\n",
      "tokenizer = XLMRobertaTokenizer.from_pretrained(\"alexandrainst/da-ned-base\")\n",
      "```\n",
      "\n",
      "The tokenizer takes 2 strings has input: the sentence and the knowledge graph (KG) context. \n",
      "Here is an example:\n",
      "```python\n",
      "sentence = \"Karen Blixen vendte tilbage til Danmark, hvor hun boede resten af sit liv på Rungstedlund, som hun arvede efter sin mor i 1939\"\n",
      "kg_context = \"udmærkelser modtaget Kritikerprisen udmærkelser modtaget Tagea Brandts Rejselegat udmærkelser modtaget Ingenio et arti udmærkelser modtaget Holbergmedaljen udmærkelser modtaget De Gyldne Laurbær mor Ingeborg Dinesen ægtefælle Bror von Blixen-Finecke køn kvinde Commons-kategori Karen Blixen LCAuth no95003722 VIAF 90663542 VIAF 121643918 GND-identifikator 118637878 ISNI 0000 0001 2096 6265 ISNI 0000 0003 6863 4408 ISNI 0000 0001 1891 0457 fødested Rungstedlund fødested Rungsted dødssted Rungstedlund dødssted København statsborgerskab Danmark NDL-nummer 00433530 dødsdato +1962-09-07T00:00:00Z dødsdato +1962-01-01T00:00:00Z fødselsdato +1885-04-17T00:00:00Z fødselsdato +1885-01-01T00:00:00Z AUT NKC jn20000600905 AUT NKC jo2015880827 AUT NKC xx0196181 emnets hovedkategori Kategori:Karen Blixen tilfælde af menneske billede Karen Blixen cropped from larger original.jpg IMDb-identifikationsnummer nm0227598 Freebase-ID /m/04ymd8w BNF 118857710 beskæftigelse skribent beskæftigelse selvbiograf beskæftigelse novelleforfatter ...\"\n",
      "```\n",
      "\n",
      "A KG context, for a specific entity, can be generated from its Wikidata page. \n",
      "In the previous example, the KG context is a string representation of the Wikidata page of [Karen Blixen (QID=Q182804)](https://www.wikidata.org/wiki/Q182804). \n",
      "See the [DaNLP documentation](https://danlp-alexandra.readthedocs.io/en/latest/docs/tasks/ned.html#xlmr) for more details about how to generate a KG context. \n",
      "\n",
      "\n",
      "## Training Data \n",
      "\n",
      "The model has been trained on the [DaNED](https://danlp-alexandra.readthedocs.io/en/latest/docs/datasets.html#daned) and [DaWikiNED](https://danlp-alexandra.readthedocs.io/en/latest/docs/datasets.html#dawikined) datasets.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 3 documents retrieved which are relevant to the query.\n",
      "Relevant Document #3:\n",
      "Source file: model_cards/Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two/README.md, \n",
      "Content: ---\n",
      "language: en\n",
      "license: apache-2.0\n",
      "datasets:\n",
      "  - hatexplain\n",
      "---\n",
      "\n",
      "\n",
      "## Table of Contents\n",
      "- [Model Details](#model-details)\n",
      "- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n",
      "- [Uses](#uses)\n",
      "- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n",
      "- [Training](#training)\n",
      "- [Evaluation](#evaluation)\n",
      "- [Technical Specifications](#technical-specifications)\n",
      "- [Citation Information](#citation-information)\n",
      "\n",
      "## Model Details\n",
      "**Model Description:** \n",
      "The model is used for classifying a text as Abusive (Hatespeech and Offensive) or Normal. The model is trained using data from Gab and Twitter and Human Rationales were included as part of the training data to boost the performance. The model also has a rationale predictor head that can predict the rationales given an abusive sentence\n",
      "\n",
      "\n",
      "- **Developed by:** Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee \n",
      "- **Model Type:** Text Classification\n",
      "- **Language(s):**  English\n",
      "- **License:**  Apache-2.0\n",
      "- **Parent Model:** See the [BERT base uncased model](https://huggingface.co/bert-base-uncased) for more information about the BERT base model.\n",
      "- **Resources for more information:**\n",
      "  - [Research Paper](https://arxiv.org/abs/2012.10289) Accepted at AAAI 2021.\n",
      "  - [GitHub Repo with datatsets and models](https://github.com/punyajoy/HateXplain)\n",
      "\n",
      "\n",
      "                                                                                               \n",
      "## How to Get Started with the Model\n",
      "\n",
      "**Details of usage**\n",
      "\n",
      "Please use the **Model_Rational_Label** class inside [models.py](models.py) to load the models. The default prediction in this hosted inference API may be wrong due to the use of different class initialisations.\n",
      "\n",
      "```python\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "### from models.py\n",
      "from models import *\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\n",
      "model = Model_Rational_Label.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two\")\n",
      "inputs = tokenizer('He is a great guy\", return_tensors=\"pt\")\n",
      "prediction_logits, _ = model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'])\n",
      "```\n",
      "\n",
      "## Uses\n",
      "\n",
      "#### Direct Use\n",
      "\n",
      "This model can be used for Text Classification\n",
      "\n",
      "\n",
      "#### Downstream Use\n",
      "\n",
      "[More information needed]\n",
      "\n",
      "#### Misuse and Out-of-scope Use\n",
      "\n",
      "The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n",
      "\n",
      "## Risks, Limitations and Biases\n",
      "\n",
      "**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n",
      "\n",
      "Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).\n",
      "\n",
      "(and if you can generate an example of a biased prediction, also something like this): \n",
      "\n",
      "Predictions generated by the model can include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups. For ![example:](https://github.com/hate-alert/HateXplain/blob/master/Figures/dataset_example.png) \n",
      "\n",
      "The model author's also note in their HateXplain paper that they \n",
      ">  *have not considered any external context such as profile bio, user gender, history of posts etc., which might be helpful in the classification task. Also, in this work we have focused on the English language. It does not consider multilingual hate speech into account.*\n",
      "\n",
      "\n",
      "#### Training Procedure\n",
      "\n",
      "##### Preprocessing\n",
      "\n",
      "The authors detail their preprocessing procedure in the [Github repository](https://github.com/hate-alert/HateXplain/tree/master/Preprocess)\n",
      "\n",
      "\n",
      "## Evaluation\n",
      "The mode authors detail the Hidden layer size and attention for the HateXplain fien tuned models in the [associated paper](https://arxiv.org/pdf/2012.10289.pdf)\n",
      "\n",
      "#### Results \n",
      "\n",
      "The model authors both in their paper and in the git repository provide the illustrative output of the BERT - HateXplain in comparison to BERT and and other HateXplain fine tuned ![models]( https://github.com/hate-alert/HateXplain/blob/master/Figures/bias-subgroup.pdf)\n",
      "\n",
      "## Citation Information\n",
      "\n",
      "```bibtex\n",
      "@article{mathew2020hatexplain,\n",
      "  title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},\n",
      "  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},\n",
      "  journal={arXiv preprint arXiv:2012.10289},\n",
      "  year={2020}\n",
      "\n",
      "}\n",
      "```\n",
      "----------------------------------------------------------------------------------------------------\n",
      "There are 3 documents retrieved which are relevant to the query.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')\n",
    "print(\"*\" * 100)\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"Relevant Document #{i+1}:\\nSource file: {doc.metadata['source']}, \\nContent: {doc.page_content}\")\n",
    "    print(\"-\"*100)\n",
    "    print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be8f3e-fc46-43fa-8a94-b3b4f4575a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5a4d1-aa1f-4520-9991-e4eb9ba1690d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839c15b-b645-4f60-855e-3c765b8f2851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409726e-4ce9-495a-b424-cbf1be5a49e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "715aa867-14e2-4003-aae7-27dd7a74a287",
   "metadata": {},
   "source": [
    "# Step 2: split the best retrieved models into markdown sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "33b43e42-2ee7-48e0-933f-579e52010ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='# Intro  \\n## History  \\nMarkdown[9] is a lightweight markup language for creating formatted text using a plain-text editor.\\nJohn Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \\nMarkdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.' metadata={'Header 1': 'Intro', 'Header 2': 'History'}\n"
     ]
    }
   ],
   "source": [
    "markdown_document = \"\"\"# Intro \\n\\n    ## History \\n\\n Markdown[9] is a lightweight markup language for creating formatted text using a plain-text editor. \n",
    "John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \n",
    "\\n\\n Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \n",
    "\\n\\n ## Rise and divergence \n",
    "\\n\\n As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for \n",
    "\\n\\n additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \n",
    "\\n\\n #### Standardization \n",
    "\\n\\n From 2012, a group of people, including Jeff Atwood and John MacFarlane, \n",
    "launched what Atwood characterised as a standardisation effort. \n",
    "\\n\\n ## Implementations \\n\\n Implementations of Markdown are available for over a dozen programming languages.\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "# MD splits\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "print(md_header_splits[0])#.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aac41e-70e0-49bf-81a3-6c9ba109cc01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
